import cv2
import mediapipe as mp
import numpy as np
import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration, WebRtcMode
import av
import math
import logging

# åˆå§‹åŒ–æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)

# WebRTC é…ç½®
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

class HandGestureTransformer(VideoTransformerBase):
    def __init__(self):
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            max_num_hands=1,
            model_complexity=0,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.7
        )
        self.mp_draw = mp.solutions.drawing_utils
        self.tip_ids = [4, 8, 12, 16, 20]
        self.vol_history = []
        self.smooth_factor = 5
        self.min_dist = 30
        self.max_dist = 300
        self.volume = 50  # é»˜è®¤éŸ³é‡

    def transform(self, frame):
        try:
            img = frame.to_ndarray(format="bgr24")
            img = cv2.resize(img, (640, 480))
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            results = self.hands.process(img_rgb)

            if results.multi_hand_landmarks:
                for hand_landmarks in results.multi_hand_landmarks:
                    self.mp_draw.draw_landmarks(
                        img, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)

                    # è·å–å…³é”®ç‚¹åæ ‡
                    lm_list = []
                    for id, lm in enumerate(hand_landmarks.landmark):
                        h, w, c = img.shape
                        cx, cy = int(lm.x * w), int(lm.y * h)
                        lm_list.append([id, cx, cy])

                    # è®¡ç®—æ‹‡æŒ‡å’Œé£ŸæŒ‡è·ç¦»
                    if len(lm_list) >= 9:
                        x1, y1 = lm_list[4][1], lm_list[4][2]
                        x2, y2 = lm_list[8][1], lm_list[8][2]
                        length = math.hypot(x2 - x1, y2 - y1)

                        # å¹³æ»‘å¤„ç†
                        vol = np.interp(length, [self.min_dist, self.max_dist], [0, 100])
                        self.vol_history.append(vol)
                        if len(self.vol_history) > self.smooth_factor:
                            self.vol_history.pop(0)
                        self.volume = sum(self.vol_history) / len(self.vol_history)

            # æ˜¾ç¤ºéŸ³é‡ä¿¡æ¯
            cv2.putText(img, f'Volume: {int(self.volume)}%', (20, 40),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            # ç»˜åˆ¶éŸ³é‡æ¡
            bar_width, bar_height = 25, 150
            bar_x, bar_y = 20, 60
            vol_fill_height = int(np.interp(self.volume, [0, 100], [0, bar_height]))
            
            cv2.rectangle(img, (bar_x, bar_y),
                         (bar_x + bar_width, bar_y + bar_height),
                         (0, 255, 0), 1)
            cv2.rectangle(img, (bar_x, bar_y + bar_height - vol_fill_height),
                         (bar_x + bar_width, bar_y + bar_height),
                         (0, 255, 0), cv2.FILLED)

            return img
        except Exception as e:
            logging.error(f"è§†é¢‘å¤„ç†é”™è¯¯: {str(e)}")
            return frame

def main():
    st.title("æ‰‹åŠ¿æ§åˆ¶éŸ³é‡æ¼”ç¤º")
    st.markdown("""
    ### ä½¿ç”¨è¯´æ˜ï¼š
    1. å…è®¸æµè§ˆå™¨è®¿é—®æ‘„åƒå¤´ã€‚
    2. æ‰‹åŠ¿æ§åˆ¶ï¼š
       - ğŸ‘† æ‹‡æŒ‡å’Œé£ŸæŒ‡åˆ†å¼€ï¼šå¢å¤§éŸ³é‡
       - ğŸ¤ æ‹‡æŒ‡å’Œé£ŸæŒ‡é è¿‘ï¼šå‡å°éŸ³é‡
    *æ³¨æ„ï¼šæ­¤æ¼”ç¤ºæ˜¾ç¤ºéŸ³é‡æ§åˆ¶ç•Œé¢ï¼Œå®é™…éŸ³é‡æ§åˆ¶éœ€è¦åœ¨æœ¬åœ°ç¯å¢ƒä¸­å®ç°ã€‚*
    """)

    # åˆå§‹åŒ–WebRTCæµ
    webrtc_ctx = webrtc_streamer(
        key="hand-gesture",
        mode=WebRtcMode.SENDRECV,
        rtc_configuration=RTC_CONFIGURATION,
        video_processor_factory=HandGestureTransformer,
        media_stream_constraints={"video": True, "audio": False},
        async_processing=True,
    )

    if st.button("é‡ç½®åº”ç”¨"):
        st.experimental_rerun()

if __name__ == "__main__":
    main()
